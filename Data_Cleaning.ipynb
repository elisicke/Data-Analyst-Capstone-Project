{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to csv files\n",
    "accounts = './data/accounts.csv'\n",
    "playbacks = './data/playbacks.csv'\n",
    "subscriptions = './data/subscriptions.csv'\n",
    "vouchers = './data/promo_vouchers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the get_engine function from our sql_functions.\n",
    "from sql_functions import get_engine #adjust this as necessary to match your sql_functions.py connection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCOUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read accounts (21.10.2020 - 01.10.2022)\n",
    "df_accounts = pd.read_csv(accounts)\n",
    "# set column names to lowercase\n",
    "df_accounts.columns = df_accounts.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non numeric characters\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code'].str.replace('-', '')\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code_clean'].str.extract('(\\d+)')\n",
    "# fill null-values with 0\n",
    "df_accounts['postal_code_clean'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type to integer\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code_clean'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping plz_files to accounts table for further geographical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file paths\n",
    "plz_ch = './data/plz_verzeichnis_ch.csv'\n",
    "plz_kanton = './data/plz_kantone_ch.csv'\n",
    "plz_de = './data/plz_verzeichnis_de.csv'\n",
    "plz_at = './data/plz_verzeichnis_at.csv'\n",
    "# read csv files\n",
    "df_plz_ch = pd.read_csv(plz_ch, sep=';')\n",
    "df_plz_kanton = pd.read_csv(plz_kanton, sep=';')\n",
    "df_plz_de = pd.read_csv(plz_de, sep=',')\n",
    "df_plz_at = pd.read_csv(plz_at, sep=';')\n",
    "# set column names to lowercase\n",
    "df_plz_ch.columns = df_plz_ch.columns.str.lower()\n",
    "df_plz_kanton.columns = df_plz_kanton.columns.str.lower()\n",
    "df_plz_de.columns = df_plz_de.columns.str.lower()\n",
    "df_plz_at.columns = df_plz_at.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean plz_kanton\n",
    "# only keep relevant columns, rename\n",
    "df_plz_kanton = df_plz_kanton[['postleitzahl / code postal / codice postale', 'ort / ville / città', 'kanton']]\n",
    "df_plz_kanton.rename(columns = {'postleitzahl / code postal / codice postale':'postal_code', 'ort / ville / città':'city', 'kanton':'state'}, inplace = True)\n",
    "df_plz_kanton.drop_duplicates(inplace = True)\n",
    "# add country_code for differentiation\n",
    "df_plz_kanton['country_code'] = 'CH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean plz_de\n",
    "# only keep relevant columns, rename, drop duplicates\n",
    "df_plz_de = df_plz_de[['plz', 'ort', 'bundesland']]\n",
    "df_plz_de.rename(columns = {'plz':'postal_code', 'ort':'city', 'bundesland':'state'}, inplace = True)\n",
    "df_plz_de.drop_duplicates(inplace = True)\n",
    "# add country_code for differentiation\n",
    "df_plz_de['country_code'] = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean plz_at\n",
    "# only keep relevant columns, rename, drop duplicates\n",
    "df_plz_at = df_plz_at[['plz', 'ort', 'bundesland']]\n",
    "df_plz_at.rename(columns = {'plz':'postal_code', 'ort':'city', 'bundesland':'state'}, inplace = True)\n",
    "df_plz_at.drop_duplicates(inplace = True)\n",
    "# add country_code for differentiation\n",
    "df_plz_at['country_code'] = 'AT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/szrk2v_52_bgyh57zj48fk400000gn/T/ipykernel_98153/1600884800.py:4: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_plz_all = df_plz_all.groupby(['postal_code'])['city', 'state', 'country_code'].first().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# unify for merging, check shape\n",
    "df_plz_all = pd.concat([df_plz_kanton, df_plz_de, df_plz_at])\n",
    "# dropping plz duplicates with multiple city, keeping the first entry each\n",
    "df_plz_all = df_plz_all.groupby(['postal_code'])['city', 'state', 'country_code'].first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge city and state information to accounts table on plz and country code\n",
    "df_accounts = pd.merge(df_accounts, df_plz_all, left_on=['postal_code_clean', 'country_code'], right_on=['postal_code', 'country_code'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/szrk2v_52_bgyh57zj48fk400000gn/T/ipykernel_98153/101013062.py:8: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_plz_ch = df_plz_ch.groupby(['postal_code'])['sprachcode', 'country_code'].first().reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postal_code</th>\n",
       "      <th>sprachcode</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>2</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>2</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   postal_code  sprachcode country_code\n",
       "0         1000           2           CH\n",
       "1         1001           2           CH\n",
       "2         1002           2           CH\n",
       "3         1003           2           CH\n",
       "4         1004           2           CH"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3488, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nMapping of the language code:\\n#1 = German  \\n#2 = French  \\n#3 = Italian \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean plz_a for language information\n",
    "# only keep relevant columns, rename, drop duplicates\n",
    "df_plz_ch = df_plz_ch[['postleitzahl', 'sprachcode']]\n",
    "df_plz_ch.rename(columns = {'postleitzahl':'postal_code'}, inplace = True)\n",
    "# add country_code for differentiation\n",
    "df_plz_ch['country_code'] = 'CH'\n",
    "#drop duplicates for dual-language-cities and keep first entry\n",
    "df_plz_ch = df_plz_ch.groupby(['postal_code'])['sprachcode', 'country_code'].first().reset_index()\n",
    "\n",
    "'''\n",
    "Mapping of the language code:\n",
    "#1 = German  \n",
    "#2 = French  \n",
    "#3 = Italian \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge language code to accounts table\n",
    "df_accounts = pd.merge(df_accounts, df_plz_ch, left_on=['postal_code_clean', 'country_code'], right_on=['postal_code', 'country_code'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate postal code columns\n",
    "df_accounts = df_accounts.drop(['postal_code_x', 'postal_code_y', 'postal_code'], axis=1)\n",
    "#rename original postal code column\n",
    "df_accounts.rename(columns = {'postal_code_x':'postal_code_original', 'city_x':'city_original', 'city_y':'city_clean', 'sprachcode': 'language_code'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country_Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add country name and region information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add country information\n",
    "country= './data/country_code.csv'\n",
    "df_country = pd.read_csv(country)\n",
    "# make column names lowercase\n",
    "df_country.columns = df_country.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep relevant columns, rename\n",
    "df_country = df_country[['name', 'alpha-2', 'region', 'sub-region']]\n",
    "df_country.rename(columns = {'alpha-2':'country_code', 'name':'country_name', 'sub-region':'sub_region'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge to accounts_new table\n",
    "df_accounts = pd.merge(df_accounts, df_country, on='country_code', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill null values\n",
    "df_accounts['language'].fillna('na', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAYBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read playbacks\n",
    "df_playbacks = pd.read_csv(playbacks)\n",
    "# set column names to lowercase\n",
    "df_playbacks.columns = df_playbacks.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing datatypes and adding column playback_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing datatype to datetime\n",
    "df_playbacks['date_start'] = pd.to_datetime(df_playbacks['date_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding playback_ID column and adding incremental nr as playback_ID to every row starting from the total count of rows, descending\n",
    "df_playbacks.insert(0, 'playback_ID', range(len(df_playbacks), 0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add category according to user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playback_ID</th>\n",
       "      <th>subscription_key</th>\n",
       "      <th>account_key</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>date_start</th>\n",
       "      <th>playback_time</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>ip_hash</th>\n",
       "      <th>app_user</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>desktop</th>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "      <td>116769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile</th>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "      <td>22800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         playback_ID  subscription_key  account_key  movie_id  date_start  \\\n",
       "device                                                                      \n",
       "desktop       116769            116769       116769    116769      116769   \n",
       "mobile         22800             22800        22800     22800       22800   \n",
       "\n",
       "         playback_time  user_agent  ip_hash  app_user  \n",
       "device                                                 \n",
       "desktop         116769      116769   116769    116769  \n",
       "mobile           22800       22800    22800     22800  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the infos about device of playback and app users\n",
    "df_playbacks.loc[df_playbacks['user_agent'].str.contains('Windows|Macintosh|TV|Linux|Darwin|CrOS|PlayStation|FreeBSD'), 'device'] = 'desktop'\n",
    "df_playbacks.loc[df_playbacks['user_agent'].str.contains('Android|iOS|iPhone|iPad'), 'device'] = 'mobile'\n",
    "df_playbacks.loc[df_playbacks['user_agent'].str.contains('filmingo'), 'app_user'] = 'yes'\n",
    "df_playbacks.loc[~df_playbacks['user_agent'].str.contains('filmingo'), 'app_user'] = 'no'\n",
    "df_playbacks.groupby('device').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBSCRIPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read subscriptions\n",
    "df_subscriptions = pd.read_csv(subscriptions)\n",
    "# set column names to lowercase\n",
    "df_subscriptions.columns = df_subscriptions.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set data types for subscription dates to datetime\n",
    "df_subscriptions['subscription_start'] = pd.to_datetime(df_subscriptions['subscription_start'])\n",
    "df_subscriptions['subscription_end'] = pd.to_datetime(df_subscriptions['subscription_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"wrong\" subscription type line (FULLACCESS - unknown type to us - only one line therefore decided to drop)\n",
    "df_subscriptions.drop(df_subscriptions[(df_subscriptions['subscription_type'] == 'FULLACCESS')].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column and calculate subscription duration\n",
    "df_subscriptions['subscription_months_raw'] = ((df_subscriptions.subscription_end) - df_subscriptions.subscription_start)/np.timedelta64(1, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column for rounded subscription months for easier further processing\n",
    "#generally round up from 0.1 to be able to allow some discrepancies due to day to day calculation of subscription duration (deduct 0.1 to be able to use .ceil)\n",
    "df_subscriptions['subscription_months'] = df_subscriptions['subscription_months_raw'] - 0.1\n",
    "df_subscriptions['subscription_months'] = df_subscriptions['subscription_months'].apply(np.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two columns for chf and eur based on the subscription_type and prices from the filmingo website\n",
    "\n",
    "# create a list of the conditions\n",
    "conditions = [\n",
    "    ((df_subscriptions['subscription_type'] == 'BASIC') & (df_subscriptions['subscription_monthly'] == 0)),\n",
    "    ((df_subscriptions['subscription_type'] == 'BASIC') & (df_subscriptions['subscription_monthly'] == 1)),\n",
    "    ((df_subscriptions['subscription_type'] == 'STANDARD') & (df_subscriptions['subscription_monthly'] == 0)),\n",
    "    ((df_subscriptions['subscription_type'] == 'STANDARD') & (df_subscriptions['subscription_monthly'] == 1)),\n",
    "    ((df_subscriptions['subscription_type'] == 'PATRON') & (df_subscriptions['subscription_monthly'] == 0))\n",
    "\n",
    "]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values_chf = ['90.0', '9.0', '150.0', '15.0', '240.0']\n",
    "values_eur = ['75.0', '7.5', '125.0', '12.5', '200.0']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "df_subscriptions['price_chf'] = np.select(conditions, values_chf)\n",
    "df_subscriptions['price_eur'] = np.select(conditions, values_eur)\n",
    "\n",
    "#change datatype into float for further calculation\n",
    "df_subscriptions['price_chf'] = df_subscriptions.price_chf.astype('float')\n",
    "df_subscriptions['price_eur'] = df_subscriptions.price_eur.astype('float')\n",
    "\n",
    "# decided to use these prices for all subscriptions regardless if they might have a different prices in the list (possibly due to discounts, total lines of abnormal prices: 39) or are gifted subscription (price: NaN, total lines 1.636)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total price per subscription (price / 12 * subscription months)\n",
    "df_subscriptions['total_price_chf'] = df_subscriptions['price_chf'] / 12 * df_subscriptions['subscription_months']\n",
    "df_subscriptions['total_price_eur'] = df_subscriptions['price_eur'] / 12 * df_subscriptions['subscription_months']\n",
    "\n",
    "\n",
    "# conditional calculation for exceptions:\n",
    "\n",
    "# if the subscription is monthly only calculate price * months\n",
    "df_subscriptions.loc[(df_subscriptions['subscription_monthly'] == 1), 'total_price_chf'] = (df_subscriptions['price_chf'] * df_subscriptions['subscription_months'])\n",
    "df_subscriptions.loc[(df_subscriptions['subscription_monthly'] == 1), 'total_price_eur'] = (df_subscriptions['price_eur'] * df_subscriptions['subscription_months'])\n",
    "\n",
    "# if the subscription is gifted & 6 months long, a different price is applicable (there is only a 6 month subscription available for gifted subscriptions)\n",
    "df_subscriptions.loc[((df_subscriptions['gift_subscription'] == True) & (df_subscriptions['subscription_months'] == 6)), 'total_price_chf'] = '49'\n",
    "df_subscriptions.loc[((df_subscriptions['gift_subscription'] == True) & (df_subscriptions['subscription_months'] == 6)), 'total_price_eur'] = '41'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOUCHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read promo_couchers\n",
    "df_vouchers = pd.read_csv(vouchers, sep=';')\n",
    "# set column names to lowercase\n",
    "df_vouchers.columns = df_vouchers.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing datatype to datetime\n",
    "df_vouchers['creationdate'] = pd.to_datetime(df_vouchers['creationdate'])\n",
    "df_vouchers['expirationdate'] = pd.to_datetime(df_vouchers['expirationdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut off time of creation -> time not relevant? \n",
    "#df_vouchers['creationdate'] = df_vouchers['creationdate'].dt.date -> does not work very well as it returns an object\n",
    "df_vouchers['creationdate'] = df_vouchers['creationdate'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding playback_ID column and adding incremental nr as playback_ID to every row starting from 1, ascending\n",
    "df_vouchers.insert(0, 'voucher_ID', range(1, 1 + len(df_vouchers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df_vouchers.rename(columns = {'account_key':'account_key_sender', 'email_hash':'email_hash_receiver', 'voucherused': 'voucher_used', 'creationdate': 'creation_date', 'expirationdate': 'expiration_date'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUSH DATA TO SQL SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'capstone_filmingo' # UPDATE 'TABLE_SCHEMA' based on schema used in class \n",
    "engine = get_engine() # assign engine to be able to query against the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accounts table was imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_name = 'accounts'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        df_accounts.to_sql(name=table_name, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_name} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_name = 'subscriptions'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        df_subscriptions.to_sql(name=table_name, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_name} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_name = 'playbacks'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        df_playbacks.to_sql(name=table_name, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_name} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promo Voucher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# Example: flights_sp for Sina's flights table, flights_groupname or similar\n",
    "table_name = 'vouchers'\n",
    "# If the specified table doesn't exist yet, it will be created\n",
    "# With 'replace', your data will be replaced if the table already exists.\n",
    "# This may take some time ...\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        df_vouchers.to_sql(name=table_name, # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        schema=schema, # Use schmea that was defined earlier\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_name} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure: Check if the number of rows match\n",
    "table_name_sql = f'''SELECT count(*) \n",
    "                    FROM {schema}.{table_name}\n",
    "                    '''\n",
    "engine.execute(table_name_sql).fetchall()[0][0] == df_vouchers.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nf_sql')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a81d278bed5b5b59425dcb5a82ce505657686243c184b4a6b67e69d01c4d432e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
