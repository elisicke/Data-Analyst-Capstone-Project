{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the get_engine function from our sql_functions.\n",
    "from sql_functions import get_engine #adjust this as necessary to match your sql_functions.py connection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to csv files\n",
    "accounts = './data/accounts.csv'\n",
    "playbacks = './data/playbacks.csv'\n",
    "subscriptions = './data/subscriptions.csv'\n",
    "vouchers = './data/promo_vouchers.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCOUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read accounts (21.10.2020 - 01.10.2022)\n",
    "df_accounts = pd.read_csv(accounts)\n",
    "# set column names to lowercase\n",
    "df_accounts.columns = df_accounts.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing datatype to datetime\n",
    "df_accounts['registration_date'] = pd.to_datetime(df_accounts['registration_date'])\n",
    "df_accounts['lastlogin_date'] = pd.to_datetime(df_accounts['lastlogin_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non numeric characters\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code'].str.replace('-', '')\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code_clean'].str.extract('(\\d+)')\n",
    "# Fill null-values with 0\n",
    "df_accounts['postal_code_clean'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type to integer\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code_clean'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping plz_files to accounts table for further geographical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "plz_ch = './data/plz_verzeichnis_ch.csv'\n",
    "plz_kanton = './data/plz_kantone_ch.csv'\n",
    "plz_de = './data/plz_verzeichnis_de.csv'\n",
    "plz_at = './data/plz_verzeichnis_at.csv'\n",
    "# Read csv files\n",
    "df_plz_ch = pd.read_csv(plz_ch, sep=';')\n",
    "df_plz_kanton = pd.read_csv(plz_kanton, sep=';')\n",
    "df_plz_de = pd.read_csv(plz_de, sep=',')\n",
    "df_plz_at = pd.read_csv(plz_at, sep=';')\n",
    "# Set column names to lowercase\n",
    "df_plz_ch.columns = df_plz_ch.columns.str.lower()\n",
    "df_plz_kanton.columns = df_plz_kanton.columns.str.lower()\n",
    "df_plz_de.columns = df_plz_de.columns.str.lower()\n",
    "df_plz_at.columns = df_plz_at.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean plz_kanton\n",
    "# Only keep relevant columns, rename\n",
    "df_plz_kanton = df_plz_kanton[['postleitzahl / code postal / codice postale', 'ort / ville / città', 'kanton']]\n",
    "df_plz_kanton.rename(columns = {'postleitzahl / code postal / codice postale':'postal_code', 'ort / ville / città':'city', 'kanton':'state'}, inplace = True)\n",
    "df_plz_kanton.drop_duplicates(inplace = True)\n",
    "# Add country_code for differentiation\n",
    "df_plz_kanton['country_code'] = 'CH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean plz_de\n",
    "# Only keep relevant columns, rename, drop duplicates\n",
    "df_plz_de = df_plz_de[['plz', 'ort', 'bundesland']]\n",
    "df_plz_de.rename(columns = {'plz':'postal_code', 'ort':'city', 'bundesland':'state'}, inplace = True)\n",
    "df_plz_de.drop_duplicates(inplace = True)\n",
    "# Add country_code for differentiation\n",
    "df_plz_de['country_code'] = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean plz_at\n",
    "# Only keep relevant columns, rename, drop duplicates\n",
    "df_plz_at = df_plz_at[['plz', 'ort', 'bundesland']]\n",
    "df_plz_at.rename(columns = {'plz':'postal_code', 'ort':'city', 'bundesland':'state'}, inplace = True)\n",
    "df_plz_at.drop_duplicates(inplace = True)\n",
    "# Add country_code for differentiation\n",
    "df_plz_at['country_code'] = 'AT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify for merging, check shape\n",
    "df_plz_all = pd.concat([df_plz_kanton, df_plz_de, df_plz_at])\n",
    "# Dropping plz duplicates with multiple city, keeping the first entry each\n",
    "df_plz_all = df_plz_all.groupby(['postal_code'])['city', 'state', 'country_code'].first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge city and state information to accounts table on plz and country code\n",
    "df_accounts = pd.merge(df_accounts, df_plz_all, left_on=['postal_code_clean', 'country_code'], right_on=['postal_code', 'country_code'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean plz_a for language information\n",
    "# Only keep relevant columns, rename, drop duplicates\n",
    "df_plz_ch = df_plz_ch[['postleitzahl', 'sprachcode']]\n",
    "df_plz_ch.rename(columns = {'postleitzahl':'postal_code'}, inplace = True)\n",
    "# Add country_code for differentiation\n",
    "df_plz_ch['country_code'] = 'CH'\n",
    "# Drop duplicates for dual-language-cities and keep first entry\n",
    "df_plz_ch = df_plz_ch.groupby(['postal_code'])['sprachcode', 'country_code'].first().reset_index()\n",
    "\n",
    "'''\n",
    "Mapping of the language code:\n",
    "#1 = German  \n",
    "#2 = French  \n",
    "#3 = Italian \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge language code to accounts table\n",
    "df_accounts = pd.merge(df_accounts, df_plz_ch, left_on=['postal_code_clean', 'country_code'], right_on=['postal_code', 'country_code'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate postal code columns\n",
    "df_accounts = df_accounts.drop(['postal_code_x', 'postal_code_y', 'postal_code'], axis=1)\n",
    "# Rename original postal code column\n",
    "df_accounts.rename(columns = {'postal_code_x':'postal_code_original', 'city_x':'city_original', 'city_y':'city_clean', 'sprachcode': 'language_code'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country_Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add country name and region information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add country information\n",
    "country= './data/country_code.csv'\n",
    "df_country = pd.read_csv(country)\n",
    "# Make column names lowercase\n",
    "df_country.columns = df_country.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep relevant columns, rename\n",
    "df_country = df_country[['name', 'alpha-2', 'region', 'sub-region']]\n",
    "df_country.rename(columns = {'alpha-2':'country_code', 'name':'country_name', 'sub-region':'sub_region'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to accounts_new table\n",
    "df_accounts = pd.merge(df_accounts, df_country, on='country_code', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values\n",
    "df_accounts['language'].fillna('na', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAYBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read playbacks\n",
    "df_playbacks = pd.read_csv(playbacks)\n",
    "# Set column names to lowercase\n",
    "df_playbacks.columns = df_playbacks.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing datatypes and adding column playback_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "df_playbacks.rename(columns = {'date_start':'datetime_start'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing datatype to datetime\n",
    "df_playbacks['datetime_start'] = pd.to_datetime(df_playbacks['datetime_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding date only column\n",
    "df_playbacks['date_start'] = pd.to_datetime(df_playbacks['datetime_start']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing datatype to datetime\n",
    "df_playbacks['date_start'] = pd.to_datetime(df_playbacks['date_start']).dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playbacks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding playback_ID column and adding incremental nr as playback_id to every row starting from the total count of rows, descending\n",
    "df_playbacks.insert(0, 'playback_id', range(len(df_playbacks), 0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add category according to user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the infos about device of playback and app users\n",
    "df_playbacks.loc[df_playbacks['user_agent'].str.contains('Windows|Macintosh|TV|Linux|Darwin|CrOS|PlayStation|FreeBSD'), 'device'] = 'desktop'\n",
    "df_playbacks.loc[df_playbacks['user_agent'].str.contains('Android|iOS|iPhone|iPad'), 'device'] = 'mobile'\n",
    "df_playbacks.loc[df_playbacks['user_agent'].str.contains('filmingo'), 'app_user'] = 'yes'\n",
    "df_playbacks.loc[~df_playbacks['user_agent'].str.contains('filmingo'), 'app_user'] = 'no'\n",
    "df_playbacks.groupby('device').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBSCRIPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read subscriptions\n",
    "df_subscriptions = pd.read_csv(subscriptions)\n",
    "# Set column names to lowercase\n",
    "df_subscriptions.columns = df_subscriptions.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types for subscription dates to datetime\n",
    "df_subscriptions['subscription_start'] = pd.to_datetime(df_subscriptions['subscription_start'])\n",
    "df_subscriptions['subscription_end'] = pd.to_datetime(df_subscriptions['subscription_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"wrong\" subscription type line (FULLACCESS - unknown type to us - only one line therefore decided to drop)\n",
    "df_subscriptions.drop(df_subscriptions[(df_subscriptions['subscription_type'] == 'FULLACCESS')].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column and calculate subscription duration\n",
    "df_subscriptions['subscription_months_raw'] = ((df_subscriptions.subscription_end) - df_subscriptions.subscription_start)/np.timedelta64(1, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column for rounded subscription months for easier further processing\n",
    "# Generally round up from 0.1 to be able to allow some discrepancies due to day to day calculation of subscription duration (deduct 0.1 to be able to use .ceil)\n",
    "df_subscriptions['subscription_months'] = df_subscriptions['subscription_months_raw'] - 0.1\n",
    "df_subscriptions['subscription_months'] = df_subscriptions['subscription_months'].apply(np.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two columns for chf and eur based on the subscription_type and prices from the filmingo website\n",
    "\n",
    "# Create a list of the conditions\n",
    "conditions = [\n",
    "    ((df_subscriptions['subscription_type'] == 'BASIC') & (df_subscriptions['subscription_monthly'] == 0)),\n",
    "    ((df_subscriptions['subscription_type'] == 'BASIC') & (df_subscriptions['subscription_monthly'] == 1)),\n",
    "    ((df_subscriptions['subscription_type'] == 'STANDARD') & (df_subscriptions['subscription_monthly'] == 0)),\n",
    "    ((df_subscriptions['subscription_type'] == 'STANDARD') & (df_subscriptions['subscription_monthly'] == 1)),\n",
    "    ((df_subscriptions['subscription_type'] == 'PATRON') & (df_subscriptions['subscription_monthly'] == 0))\n",
    "\n",
    "]\n",
    "\n",
    "# Create a list of the values we want to assign for each condition\n",
    "values_chf = ['90.0', '9.0', '150.0', '15.0', '240.0']\n",
    "values_eur = ['75.0', '7.5', '125.0', '12.5', '200.0']\n",
    "\n",
    "# Create a new column and use np.select to assign values to it using our lists as arguments\n",
    "df_subscriptions['price_chf'] = np.select(conditions, values_chf)\n",
    "df_subscriptions['price_eur'] = np.select(conditions, values_eur)\n",
    "\n",
    "# Change datatype into float for further calculation\n",
    "df_subscriptions['price_chf'] = df_subscriptions.price_chf.astype('float')\n",
    "df_subscriptions['price_eur'] = df_subscriptions.price_eur.astype('float')\n",
    "\n",
    "# Decided to use these prices for all subscriptions regardless if they might have a different prices in the list or are gifted subscription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total price per subscription (price / 12 * subscription months)\n",
    "df_subscriptions['total_price_chf'] = df_subscriptions['price_chf'] / 12 * df_subscriptions['subscription_months']\n",
    "df_subscriptions['total_price_eur'] = df_subscriptions['price_eur'] / 12 * df_subscriptions['subscription_months']\n",
    "\n",
    "\n",
    "# Conditional calculation for exceptions:\n",
    "\n",
    "# If the subscription is monthly only calculate price * months\n",
    "df_subscriptions.loc[(df_subscriptions['subscription_monthly'] == 1), 'total_price_chf'] = (df_subscriptions['price_chf'] * df_subscriptions['subscription_months'])\n",
    "df_subscriptions.loc[(df_subscriptions['subscription_monthly'] == 1), 'total_price_eur'] = (df_subscriptions['price_eur'] * df_subscriptions['subscription_months'])\n",
    "\n",
    "# If the subscription is gifted & 6 months long, a different price is applicable (there is only a 6 month subscription available for gifted subscriptions)\n",
    "df_subscriptions.loc[((df_subscriptions['gift_subscription'] == True) & (df_subscriptions['subscription_months'] == 6)), 'total_price_chf'] = '49'\n",
    "df_subscriptions.loc[((df_subscriptions['gift_subscription'] == True) & (df_subscriptions['subscription_months'] == 6)), 'total_price_eur'] = '41'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOUCHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read promo_couchers\n",
    "df_vouchers = pd.read_csv(vouchers, sep=';')\n",
    "# Set column names to lowercase\n",
    "df_vouchers.columns = df_vouchers.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing datatype to datetime\n",
    "df_vouchers['creationdate'] = pd.to_datetime(df_vouchers['creationdate'])\n",
    "df_vouchers['expirationdate'] = pd.to_datetime(df_vouchers['expirationdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut off time of creation -> time not relevant? \n",
    "# df_vouchers['creationdate'] = df_vouchers['creationdate'].dt.date -> does not work very well as it returns an object\n",
    "df_vouchers['creationdate'] = df_vouchers['creationdate'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding voucher_id column and adding incremental nr as voucher_id to every row starting from 1, ascending\n",
    "df_vouchers.insert(0, 'voucher_id', range(1, 1 + len(df_vouchers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_vouchers.rename(columns = {'account_key':'account_key_sender', 'email_hash':'email_hash_receiver', 'voucherused': 'voucher_used', 'creationdate': 'creation_date', 'expirationdate': 'expiration_date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate vouchers table to our time range\n",
    "# start_date = '2020-10-01'\n",
    "# end_date = '2022-10-01'\n",
    "# mask = (df_vouchers['creation_date'] >= start_date) & (df_vouchers['creation_date'] <= end_date)\n",
    "# df_vouchers = df_vouchers.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add voucher information to accounts table - only take into account unique email_hash_receiver (drop duplicates)\n",
    "df_accounts = df_accounts.merge(df_vouchers.drop_duplicates('email_hash_receiver')[['email_hash_receiver', 'voucher_used']], how='left', left_on='email_hash', right_on='email_hash_receiver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop email_hash_receiver as it is not needed\n",
    "df_accounts.drop('email_hash_receiver', axis=1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = pd.DataFrame({\"Date\": pd.date_range('2020-01-01','2022-12-31')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUSH DATA TO SQL SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'capstone_filmingo' # UPDATE 'TABLE_SCHEMA' based on schema used in class \n",
    "engine = get_engine() # assign engine to be able to query against the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# table_name = 'accounts'\n",
    "# # If the specified table doesn't exist yet, it will be created\n",
    "# # With 'replace', your data will be replaced if the table already exists.\n",
    "# # This may take some time ...\n",
    "\n",
    "# # Write records stored in a dataframe to SQL database\n",
    "# if engine!=None:\n",
    "#     try:\n",
    "#         df_date.to_sql(name=table_name, # Name of SQL table\n",
    "#                         con=engine, # Engine or connection\n",
    "#                         if_exists='replace', # Drop the table before inserting new values \n",
    "#                         schema=schema, # Use schmea that was defined earlier\n",
    "#                         index=False, # Write DataFrame index as a column\n",
    "#                         chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "#                         method='multi') # Pass multiple values in a single INSERT clause\n",
    "#         print(f\"The {table_name} table was imported successfully.\")\n",
    "#     # Error handling\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "#         engine = None\n",
    "# else:\n",
    "#      print('Push did not work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure: Check if the number of rows match\n",
    "table_name_sql = f'''SELECT count(*) \n",
    "                    FROM {schema}.{table_name}\n",
    "                    '''\n",
    "engine.execute(table_name_sql).fetchall()[0][0] == df_accounts.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# table_name = 'subscriptions'\n",
    "# # If the specified table doesn't exist yet, it will be created\n",
    "# # With 'replace', your data will be replaced if the table already exists.\n",
    "# # This may take some time ...\n",
    "\n",
    "# # Write records stored in a dataframe to SQL database\n",
    "# if engine!=None:\n",
    "#     try:\n",
    "#         df_date.to_sql(name=table_name, # Name of SQL table\n",
    "#                         con=engine, # Engine or connection\n",
    "#                         if_exists='replace', # Drop the table before inserting new values \n",
    "#                         schema=schema, # Use schmea that was defined earlier\n",
    "#                         index=False, # Write DataFrame index as a column\n",
    "#                         chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "#                         method='multi') # Pass multiple values in a single INSERT clause\n",
    "#         print(f\"The {table_name} table was imported successfully.\")\n",
    "#     # Error handling\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "#         engine = None\n",
    "# else:\n",
    "#      print('Push did not work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure: Check if the number of rows match\n",
    "table_name_sql = f'''SELECT count(*) \n",
    "                    FROM {schema}.{table_name}\n",
    "                    '''\n",
    "engine.execute(table_name_sql).fetchall()[0][0] == df_subscriptions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# table_name = 'playbacks'\n",
    "# # If the specified table doesn't exist yet, it will be created\n",
    "# # With 'replace', your data will be replaced if the table already exists.\n",
    "# # This may take some time ...\n",
    "\n",
    "# # Write records stored in a dataframe to SQL database\n",
    "# if engine!=None:\n",
    "#     try:\n",
    "#         df_date.to_sql(name=table_name, # Name of SQL table\n",
    "#                         con=engine, # Engine or connection\n",
    "#                         if_exists='replace', # Drop the table before inserting new values \n",
    "#                         schema=schema, # Use schmea that was defined earlier\n",
    "#                         index=False, # Write DataFrame index as a column\n",
    "#                         chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "#                         method='multi') # Pass multiple values in a single INSERT clause\n",
    "#         print(f\"The {table_name} table was imported successfully.\")\n",
    "#     # Error handling\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "#         engine = None\n",
    "# else:\n",
    "#      print('Push did not work')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure: Check if the number of rows match\n",
    "table_name_sql = f'''SELECT count(*) \n",
    "                    FROM {schema}.{table_name}\n",
    "                    '''\n",
    "engine.execute(table_name_sql).fetchall()[0][0] == df_playbacks.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promo Voucher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# table_name = 'vouchers'\n",
    "# # If the specified table doesn't exist yet, it will be created\n",
    "# # With 'replace', your data will be replaced if the table already exists.\n",
    "# # This may take some time ...\n",
    "\n",
    "# # Write records stored in a dataframe to SQL database\n",
    "# if engine!=None:\n",
    "#     try:\n",
    "#         df_date.to_sql(name=table_name, # Name of SQL table\n",
    "#                         con=engine, # Engine or connection\n",
    "#                         if_exists='replace', # Drop the table before inserting new values \n",
    "#                         schema=schema, # Use schmea that was defined earlier\n",
    "#                         index=False, # Write DataFrame index as a column\n",
    "#                         chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "#                         method='multi') # Pass multiple values in a single INSERT clause\n",
    "#         print(f\"The {table_name} table was imported successfully.\")\n",
    "#     # Error handling\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "#         engine = None\n",
    "# else:\n",
    "#      print('Push did not work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure: Check if the number of rows match\n",
    "table_name_sql = f'''SELECT count(*) \n",
    "                    FROM {schema}.{table_name}\n",
    "                    '''\n",
    "engine.execute(table_name_sql).fetchall()[0][0] == df_vouchers.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify which table within your database you want to push your data to. Give your table an unambiguous name.\n",
    "# table_name = 'date'\n",
    "# # If the specified table doesn't exist yet, it will be created\n",
    "# # With 'replace', your data will be replaced if the table already exists.\n",
    "# # This may take some time ...\n",
    "\n",
    "# # Write records stored in a dataframe to SQL database\n",
    "# if engine!=None:\n",
    "#     try:\n",
    "#         df_date.to_sql(name=table_name, # Name of SQL table\n",
    "#                         con=engine, # Engine or connection\n",
    "#                         if_exists='replace', # Drop the table before inserting new values \n",
    "#                         schema=schema, # Use schmea that was defined earlier\n",
    "#                         index=False, # Write DataFrame index as a column\n",
    "#                         chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "#                         method='multi') # Pass multiple values in a single INSERT clause\n",
    "#         print(f\"The {table_name} table was imported successfully.\")\n",
    "#     # Error handling\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "#         engine = None\n",
    "# else:\n",
    "#      print('Push did not work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure: Check if the number of rows match\n",
    "table_name_sql = f'''SELECT count(*) \n",
    "                    FROM {schema}.{table_name}\n",
    "                    '''\n",
    "engine.execute(table_name_sql).fetchall()[0][0] == df_date.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nf_sql')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a81d278bed5b5b59425dcb5a82ce505657686243c184b4a6b67e69d01c4d432e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
