{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the csv file you want to read in\n",
    "accounts = './data/accounts.csv'\n",
    "playbacks = './data/playbacks.csv'\n",
    "subscriptions = './data/subscriptions.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kanton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/plz_verzeichnis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kanton \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data/plz_verzeichnis.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_kanton \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(kanton, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m;\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#df_kanton.columns = df_cntrycd.columns.str.lower()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m display(df_kanton\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_base/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_base/lib/python3.9/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_base/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_base/lib/python3.9/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_base/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m     f,\n\u001b[1;32m   1218\u001b[0m     mode,\n\u001b[1;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1225\u001b[0m )\n\u001b[1;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_base/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/plz_verzeichnis.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "kanton = './data/plz_verzeichnis.csv'\n",
    "df_kanton = pd.read_csv(kanton, sep=';')\n",
    "#df_kanton.columns = df_cntrycd.columns.str.lower()\n",
    "display(df_kanton.shape)\n",
    "display(df_kanton.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBSCRIPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read subscriptions\n",
    "df_subscriptions = pd.read_csv(subscriptions)\n",
    "df_subscriptions.columns = df_subscriptions.columns.str.lower()\n",
    "\n",
    "display(df_subscriptions.shape)\n",
    "display(df_subscriptions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subscriptions.groupby(['currency','price']).subscription_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set data types for subscription dates to datetime\n",
    "df_subscriptions['subscription_start'] = pd.to_datetime(df_subscriptions['subscription_start'])\n",
    "df_subscriptions['subscription_end'] = pd.to_datetime(df_subscriptions['subscription_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new 'clean' DataFrame \n",
    "df_subscriptions_clean = df_subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"wrong\" subscription type line (FULLACCESS - unknown type to us - only one line therefore decided to drop)\n",
    "df_subscriptions_clean.drop(df_subscriptions_clean[(df_subscriptions['subscription_type'] == 'FULLACCESS')].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column and calculate subscription duration for calculating actual price per subscription\n",
    "df_subscriptions_clean['subscription_months_raw'] = ((df_subscriptions_clean.subscription_end) - df_subscriptions_clean.subscription_start)/np.timedelta64(1, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column with rounded subscription months\n",
    "#generally round up from 0.1 to be able to allow some discrepancies due to day to day calculation of subscription duration (deduct 0.1 to be able to use .ceil)\n",
    "df_subscriptions_clean['subscription_months'] = df_subscriptions_clean['subscription_months_raw'] - 0.1\n",
    "df_subscriptions_clean['subscription_months'] = df_subscriptions_clean['subscription_months'].apply(np.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two columns for chf and eur based on the subscription_type and prices from the filmingo website\n",
    "\n",
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    ((df_subscriptions_clean['subscription_type'] == 'BASIC') & (df_subscriptions_clean['subscription_monthly'] == 0)),\n",
    "    ((df_subscriptions_clean['subscription_type'] == 'BASIC') & (df_subscriptions_clean['subscription_monthly'] == 1)),\n",
    "    ((df_subscriptions_clean['subscription_type'] == 'STANDARD') & (df_subscriptions_clean['subscription_monthly'] == 0)),\n",
    "    ((df_subscriptions_clean['subscription_type'] == 'STANDARD') & (df_subscriptions_clean['subscription_monthly'] == 1)),\n",
    "    ((df_subscriptions_clean['subscription_type'] == 'PATRON') & (df_subscriptions_clean['subscription_monthly'] == 0))\n",
    "\n",
    "]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values_chf = ['90.0', '9.0', '150.0', '15.0', '240.0']\n",
    "values_eur = ['75.0', '7.5', '125.0', '12.5', '200.0']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "df_subscriptions_clean['price_chf'] = np.select(conditions, values_chf)\n",
    "df_subscriptions_clean['price_eur'] = np.select(conditions, values_eur)\n",
    "\n",
    "#change datatype into float for further calculation\n",
    "df_subscriptions_clean['price_chf'] = df_subscriptions_clean.price_chf.astype('float')\n",
    "df_subscriptions_clean['price_eur'] = df_subscriptions_clean.price_eur.astype('float')\n",
    "\n",
    "# decided to use these prices for all subscriptions regardless if they might have a different prices in the list (possibly due to discounts, total lines of abnormal prices: 39) or are gifted subscription (price: NaN, total lines 1.636)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total price per subscription (price / 12 * subscription months)\n",
    "df_subscriptions_clean['total_price_chf'] = df_subscriptions_clean['price_chf'] / 12 * df_subscriptions_clean['subscription_months']\n",
    "df_subscriptions_clean['total_price_eur'] = df_subscriptions_clean['price_eur'] / 12 * df_subscriptions_clean['subscription_months']\n",
    "\n",
    "# conditional calculation for exceptions:\n",
    "\n",
    "# if the subscription is monthly only calculate price * months\n",
    "df_subscriptions_clean.loc[(df_subscriptions_clean['subscription_monthly'] == 1), 'total_price_chf'] = (df_subscriptions_clean['price_chf'] * df_subscriptions_clean['subscription_months'])\n",
    "df_subscriptions_clean.loc[(df_subscriptions_clean['subscription_monthly'] == 1), 'total_price_eur'] = (df_subscriptions_clean['price_eur'] * df_subscriptions_clean['subscription_months'])\n",
    "\n",
    "# if the subscription is gifted and 6 months long, a different price is applicable (there is only a 6 month subscription available for gifted subscriptions)\n",
    "df_subscriptions_clean.loc[((df_subscriptions_clean['gift_subscription'] == True) & (df_subscriptions_clean['subscription_months'] == 6)), 'total_price_chf'] = '49'\n",
    "df_subscriptions_clean.loc[((df_subscriptions_clean['gift_subscription'] == True) & (df_subscriptions_clean['subscription_months'] == 6)), 'total_price_eur'] = '41'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subscriptions_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "df_subscriptions_clean.query('gift_subscription == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_subscriptions_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#test\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_subscriptions_clean\u001b[39m.\u001b[39mquery(\u001b[39m'\u001b[39m\u001b[39msubscription_monthly == 1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_subscriptions_clean' is not defined"
     ]
    }
   ],
   "source": [
    "#test\n",
    "df_subscriptions_clean.query('subscription_monthly == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(gifted_df.groupby('subscription_months').max())\n",
    "print(df_subscriptions_clean.groupby(['subscription_months','subscription_monthly']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subscriptions_clean.to_csv('./data/subcriptions_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gifted_df= df_subscriptions_clean.loc[df_subscriptions_clean['gift_subscription']==True]\n",
    "# gifted_df['months'] = gifted_df['subscription_months'].round()\n",
    "# gifted_df.tail(15)\n",
    "# #display(gifted_df.groupby('subscription_months').max())\n",
    "# gifted_df.groupby('months').sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nongifted_df= df_subscriptions_clean.loc[df_subscriptions_clean['gift_subscription']==False]\n",
    "\n",
    "# nongifted_df.groupby(['months', 'subscription_monthly']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nongifted_df.query('account_key == b43a87d35bf285afdbb1c931b68ea2e6dad1f9dcc62947')\n",
    "\n",
    "#nongifted_df[nongifted_df['account_key'].str.contains('d8aa9f6793e94bc168a65808c9fe5809d4516448eae392a33edec16391c71d1e', na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subscriptions_clean = df_subscriptions_clean.drop('calc_price_chf', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCOUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read accounts (21.10.2020 - 01.10.2022)\n",
    "df_accounts = pd.read_csv(accounts)\n",
    "# set column names to lowercase\n",
    "df_accounts.columns = df_accounts.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22154, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accounts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo drop duplicates from vouchers for merge\n",
    "# add voucher information to accounts table - have to add both columns\n",
    "df_accounts_new = pd.merge(df_accounts, df_vouchers [['email_hash_receiver', 'voucher_used']], left_on='email_hash', right_on='email_hash_receiver', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non numeric characters\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code'].str.replace('-', '')\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code_clean'].str.extract('(\\d+)')\n",
    "# fill null-values with 0\n",
    "df_accounts['postal_code_clean'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type to integer\n",
    "df_accounts['postal_code_clean'] = df_accounts['postal_code_clean'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping plz_files to accounts table for further geographical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file paths\n",
    "plz_ch = './data/plz_verzeichnis_ch.csv'\n",
    "plz_kanton = './data/plz_kantone_ch.csv'\n",
    "plz_de = './data/plz_verzeichnis_de.csv'\n",
    "plz_at = './data/plz_verzeichnis_at.csv'\n",
    "# read csv files\n",
    "df_plz_ch = pd.read_csv(plz_ch, sep=';')\n",
    "df_plz_kanton = pd.read_csv(plz_kanton, sep=';')\n",
    "df_plz_de = pd.read_csv(plz_de, sep=',')\n",
    "df_plz_at = pd.read_csv(plz_at, sep=';')\n",
    "# set column names to lowercase\n",
    "df_plz_ch.columns = df_plz_ch.columns.str.lower()\n",
    "df_plz_kanton.columns = df_plz_kanton.columns.str.lower()\n",
    "df_plz_de.columns = df_plz_de.columns.str.lower()\n",
    "df_plz_at.columns = df_plz_at.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean plz_kanton\n",
    "# only keep relevant columns, rename\n",
    "df_plz_kanton = df_plz_kanton[['postleitzahl / code postal / codice postale', 'ort / ville / città', 'kanton']]\n",
    "df_plz_kanton.rename(columns = {'postleitzahl / code postal / codice postale':'postal_code', 'ort / ville / città':'city', 'kanton':'state'}, inplace = True)\n",
    "df_plz_kanton.drop_duplicates(inplace = True)\n",
    "# add country_code for differentiation\n",
    "df_plz_kanton['country_code'] = 'CH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean plz_de\n",
    "# only keep relevant columns, rename, drop duplicates\n",
    "df_plz_de = df_plz_de[['plz', 'ort', 'bundesland']]\n",
    "df_plz_de.rename(columns = {'plz':'postal_code', 'ort':'city', 'bundesland':'state'}, inplace = True)\n",
    "df_plz_de.drop_duplicates(inplace = True)\n",
    "# add country_code for differentiation\n",
    "df_plz_de['country_code'] = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean plz_at\n",
    "# only keep relevant columns, rename, drop duplicates\n",
    "df_plz_at = df_plz_at[['plz', 'ort', 'bundesland']]\n",
    "df_plz_at.rename(columns = {'plz':'postal_code', 'ort':'city', 'bundesland':'state'}, inplace = True)\n",
    "df_plz_at.drop_duplicates(inplace = True)\n",
    "# add country_code for differentiation\n",
    "df_plz_at['country_code'] = 'AT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify for merging, check shape\n",
    "df_plz_all = pd.concat([df_plz_kanton, df_plz_de, df_plz_at])\n",
    "# dropping plz duplicates with multiple city, keeping the first entry each\n",
    "df_plz_all = df_plz_all.groupby(['postal_code'])['city', 'state', 'country_code'].first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge city and state information to accounts table on plz and country code\n",
    "df_accounts = pd.merge(df_accounts, df_plz_all, left_on=['postal_code_clean', 'country_code'], right_on=['postal_code', 'country_code'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean plz_ch for language information\n",
    "# only keep relevant columns, rename, drop duplicates\n",
    "df_plz_ch = df_plz_ch[['postleitzahl', 'sprachcode']]\n",
    "df_plz_ch.rename(columns = {'postleitzahl':'postal_code'}, inplace = True)\n",
    "df_plz_ch.drop_duplicates(inplace=True)\n",
    "# add country_code for differentiation\n",
    "df_plz_ch['country_code'] = 'CH'\n",
    "\n",
    "'''\n",
    "Mapping of the language code:\n",
    "#1 = German  \n",
    "#2 = French  \n",
    "#3 = Italian \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge language code to accounts table\n",
    "df_accounts = pd.merge(df_accounts, df_plz_ch, left_on=['postal_code_clean', 'country_code'], right_on=['postal_code', 'country_code'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate postal code columns\n",
    "df_accounts = df_accounts.drop(['postal_code_x', 'postal_code_y', 'postal_code'], axis=1)\n",
    "#rename original postal code column\n",
    "df_accounts.rename(columns = {'postal_code_x':'postal_code_original', 'city_x':'city_original', 'city_y':'city_clean'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accounts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country_Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add country name and region information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add country information\n",
    "country= './data/country_code.csv'\n",
    "df_country = pd.read_csv(country)\n",
    "# make column names lowercase\n",
    "df_country.columns = df_country.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep relevant columns, rename\n",
    "df_country = df_country[['name', 'alpha-2', 'region', 'sub-region']]\n",
    "df_country.rename(columns = {'alpha-2':'country_code', 'name':'country_name', 'sub-region':'sub_region'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge to accounts_new table\n",
    "df_accounts = pd.merge(df_accounts, df_country, on='country_code', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill null values\n",
    "df_accounts['language'].fillna('na', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vouchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings summary:\n",
    "* Multiple vouchers sent to the same email ( max 14 times) -> not really idea of voucher campaign -> not a \"new customer\"  \n",
    "* most of these multiple sent vouchers come from the same account -> account sharing? -> but looks like more individual cases  \n",
    "* approx. 50% do not redeem their voucher  \n",
    "* 718 unique email_hash connected to an accounts (of 5827 -> 12%)\n",
    "\n",
    "Possible further investigation:\n",
    "* how many email_hashes which were sent only 1/2 vouchers -> more in line with campaign idea\n",
    "* the emails with multiple vouchers sent -> are they paying customer as well?\n",
    "* How many of the 718 accounts have a subscription / OTR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to csv files\n",
    "accounts = './data/accounts.csv'\n",
    "playbacks = './data/playbacks.csv'\n",
    "subscriptions = './data/subscriptions.csv'\n",
    "vouchers = './data/promo_vouchers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read promo_couchers\n",
    "df_vouchers = pd.read_csv(vouchers, sep=';')\n",
    "# set column names to lowercase\n",
    "df_vouchers.columns = df_vouchers.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing datatype to datetime\n",
    "df_vouchers['creationdate'] = pd.to_datetime(df_vouchers['creationdate'])\n",
    "df_vouchers['expirationdate'] = pd.to_datetime(df_vouchers['expirationdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut off time of creation -> time not relevant? \n",
    "#df_vouchers['creationdate'] = df_vouchers['creationdate'].dt.date -> does not work very well as it returns an object\n",
    "df_vouchers['creationdate'] = df_vouchers['creationdate'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers.groupby(['email_hash']).count().sort_values(by='account_key', ascending=False)\n",
    "# 14 times shared to same account - not idea of promo campaign -> not a \"new customer\" -> possibly only individual cases\n",
    "# ToDo: check how many cases are 1 / lower than 2 for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vouchers_acc.groupby(['email_hash']).count().sort_values(by='account_key_x', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers.groupby(['email_hash', 'account_key']).count().sort_values(by='movie_id', ascending=False)\n",
    "#most emails on top get vouchers from same account_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers.groupby(['voucherused']).count().sort_values(by='movie_id', ascending=False)\n",
    "# approx 50% do not redeem their voucher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vouchers and accounts - not used as it takes in all information from accounts (possibly needed later)\n",
    "# df_vouchers_new = df_vouchers.merge(df_accounts, how='left', on='email_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges but only add the relevant columns for easier processing (here: account_key, subcription count and OTR count)\n",
    "df_vouchers_new = pd.merge(df_vouchers, df_accounts [['email_hash', 'account_key', 'subscription_count', 'onetime_rental_count']], on='email_hash', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vouchers_acc.account_key_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vouchers_new.shape)\n",
    "df_vouchers_new['account_key_y'].isna().sum()\n",
    "# 6619 NaN (no account connected) of 7790 (so 1.171 emails with account connected)-> also be aware of multiples in vouchers.email_hash -> see code below for this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers_new.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new dataframe and drop all NaN to see only the emails with connected account for further investigation\n",
    "df_vouchers_acc = df_vouchers_new\n",
    "df_vouchers_acc.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that correct amount of rows is left\n",
    "df_vouchers_acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers_acc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_vouchers_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb Cell 76\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elisabeth/neuefische/capstone-repo/Group_Liz.ipynb#Y135sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_vouchers_acc[\u001b[39m'\u001b[39m\u001b[39memail_hash\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_vouchers_acc' is not defined"
     ]
    }
   ],
   "source": [
    "df_vouchers_acc['email_hash'].nunique()\n",
    "#718 unique email_hash/accounts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers.email_hash.nunique()\n",
    "#5827 total unique email_hashs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vouchers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge accounts and vouchers, but only add the relevant column (voucherused)\n",
    "df_accounts = pd.merge(df_accounts, df_vouchers [['email_hash', 'voucherused']], on='email_hash', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new column vouchers in accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add voucher information to accounts table - have to add both columns\n",
    "df_accounts = pd.merge(df_accounts, df_vouchers [['email_hash', 'voucherused']], left_on='email_hash', right_on='email_hash', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add voucher information to accounts table - have to add both columns\n",
    "# df_accounts = pd.merge(df_accounts, df_vouchers [['email_hash_receiver', 'voucher_used']], left_on='email_hash', right_on='email_hash_receiver', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop email_hash_receiver as it is not needed\n",
    "# df_accounts.drop('email_hash_receiver', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accounts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nf_base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab3d85a18739f6fff6a9c8c504adc2ff9340867b576dede986e2ee74c099e4e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
